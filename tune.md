

Comprehensive Fine-Tuning and Deployment Report

(For Qwen2.5-3B-Instruct, Algospeak Moderation System)

‚∏ª

1. Types of Fine-Tuning for Large Language Models

A. Full Fine-Tuning

What it is:
	‚Ä¢	Updates all the model‚Äôs weights during training.
	‚Ä¢	Requires loading the entire model in FP16 or BF16 precision.

How it‚Äôs done:
	1.	Download the full base model (e.g., Llama 3.2, 7B or 13B).
	2.	Use PyTorch/Transformers with large GPUs (A100, H100).
	3.	Train with your dataset, updating billions of parameters.

Benefits:
	‚Ä¢	Fully adapts the model to your dataset.
	‚Ä¢	Best performance for large data if you have huge compute.

Drawbacks:
	‚Ä¢	Needs 50GB+ VRAM for 13B models.
	‚Ä¢	Takes days and costs thousands of dollars.
	‚Ä¢	Overkill for a MacBook Air and a classification task like algospeak moderation.

‚∏ª

B. LoRA (Low-Rank Adaptation)

What it is:
	‚Ä¢	Keeps the base model weights frozen.
	‚Ä¢	Trains small low-rank adapter layers to capture new task knowledge.
	‚Ä¢	Adapters can be merged or used alongside the base model.

How it‚Äôs done:
	1.	Download the base model (FP16).
	2.	Load it in PyTorch (Apple MPS support exists, but it‚Äôs heavy).
	3.	Train only millions of parameters instead of billions.

Benefits:
	‚Ä¢	Much faster and cheaper than full fine-tuning.
	‚Ä¢	Adapters are tiny (MB-sized) and reusable for multiple tasks.

Drawbacks:
	‚Ä¢	Still must load full FP16 weights during training.
	‚Ä¢	On an 8GB RAM MacBook, even a 7B model can be hard to fit.
	‚Ä¢	Slower training because FP16 weights dominate memory.

‚∏ª

C. QLoRA (Quantized LoRA)

What it is:
	‚Ä¢	Loads the base model in 4-bit quantization (NF4), reducing VRAM by 3‚Äì4√ó.
	‚Ä¢	Quantized weights stay frozen (not updated).
	‚Ä¢	Trains LoRA adapters in FP16 for quality.

How it‚Äôs done:
	1.	Download the base model (FP16).
	2.	Load it quantized (4-bit) with BitsAndBytes (Colab) or Unsloth (Mac).
	3.	Train LoRA adapters.
	4.	Save adapters, then merge and quantize for deployment.

Benefits:
	‚Ä¢	Huge memory savings:
	‚Ä¢	3B models fit in ~3GB VRAM.
	‚Ä¢	7B can fit in ~6GB with optimization.
	‚Ä¢	Can train on Colab GPUs or MacBook Air (8GB RAM).
	‚Ä¢	Produces high-quality adapters even on low-resource hardware.

Drawbacks:
	‚Ä¢	Cannot fine-tune the quantized base weights (only adapters).
	‚Ä¢	Slightly slower per-step than pure FP16 training, but far more efficient overall.

‚∏ª

2. Why QLoRA is Our Only Practical Choice
	‚Ä¢	Our setup: MacBook Air (8GB RAM) for deployment, Colab (T4/A100/L4) for training.
	‚Ä¢	Full fine-tuning is impossible (VRAM limits).
	‚Ä¢	Standard LoRA is heavy (needs full FP16 weights in VRAM).
	‚Ä¢	QLoRA works everywhere:
	‚Ä¢	Loads the model in 4-bit NF4 quantization, slashing VRAM.
	‚Ä¢	Trains only LoRA adapters (tiny MB-sized).
	‚Ä¢	Fully compatible with Colab GPUs and Apple Silicon (M1).

‚∏ª

3. How It Fits Our Project (Algospeak Moderation)

We are fine-tuning to:
	‚Ä¢	Detect algospeak (slang like ‚Äúunalive‚Äù, ‚Äúseggs‚Äù).
	‚Ä¢	Normalize text (so models see ‚Äúkill‚Äù, ‚Äúsex‚Äù) and classify it as harmful or safe.
	‚Ä¢	Output confidence scores and categories (self-harm, hate, adult content).

With QLoRA:
	‚Ä¢	The 3B reasoning model fits easily on Colab GPUs and Apple M1.
	‚Ä¢	Training is fast and memory-efficient.
	‚Ä¢	After training:
	‚Ä¢	We merge the adapters with base weights.
	‚Ä¢	We quantize to GGUF (4-bit) for ultra-fast inference with Ollama or llama.cpp.

‚∏ª

4. Chosen Model: Qwen2.5-3B-Instruct

We use Qwen2.5-3B-Instruct because:
	‚Ä¢	Instruction-tuned reasoning model:
	‚Ä¢	Learns our task with 25‚Äì50% fewer steps than a base model.
	‚Ä¢	Can output short reasoning/explanations for flagged content (TrustLab-friendly).
	‚Ä¢	Same size as the base (3.09B parameters):
	‚Ä¢	~2.3 GB FP16.
	‚Ä¢	~1.2‚Äì1.5 GB RAM in 4-bit.
	‚Ä¢	Avoids extra compute needed to teach prompt-following (already trained for it).

Why Not the Base Model
	‚Ä¢	Base model is only for raw text prediction.
	‚Ä¢	Would need extra epochs to learn prompt formatting and structured outputs.
	‚Ä¢	Wastes Colab GPU time and resources.

‚∏ª

5. Fine-Tuning Process (Step by Step)

Step 1 ‚Äì Load Base Model (Quantized)
	‚Ä¢	Load Qwen2.5-3B-Instruct.
	‚Ä¢	Quantize in 4-bit NF4 using:
	‚Ä¢	BitsAndBytes (Colab) or
	‚Ä¢	Unsloth (Mac).
	‚Ä¢	Base weights remain frozen (not updated).

Step 2 ‚Äì Attach LoRA Adapters
	‚Ä¢	Add trainable low-rank layers.
	‚Ä¢	Parameters (both Colab & Mac): r=16, alpha=32, dropout=0.05.
	‚Ä¢	Unsloth and Colab handle memory optimizations automatically.

Step 3 ‚Äì Train on Our Algospeak Dataset (What We Actually Used)
	‚Ä¢	Dataset: training_dataset_colab.json (52K samples)
	‚Ä¢	Instruction format we used:

Instruction: Analyze this content for harmfulness and classify it appropriately. Consider both direct language and coded language (algospeak).
Input: I want to unalive myself
Output: extremely_harmful


	‚Ä¢	Training completed successfully using QLoRA with Unsloth
	‚Ä¢	Generated quantized model: unsloth.Q4_K_M.gguf (1.9GB)

Step 4 ‚Äì Save Outputs
	‚Ä¢	Fine-tuning produces only the LoRA adapters:

adapter_model.safetensors
adapter_config.json


	‚Ä¢	Save by:
	‚Ä¢	Direct files.download() from Colab,
	‚Ä¢	Or mount Google Drive,
	‚Ä¢	Or push to Hugging Face Hub.

‚∏ª

6. Post-Training Steps (On Mac)
	1.	Download Adapters (from Colab or Hugging Face).
	2.	Download Base FP16 Model:

git lfs install
git clone https://huggingface.co/Qwen/Qwen2.5-3B-Instruct


	3.	Merge LoRA Adapters with Base Weights ‚Üí Hugging Face model:

config.json
tokenizer.json
model.safetensors


	4.	Quantize the merged model to GGUF (4-bit) using llama.cpp tools:

fine_tuned_model.gguf


	5.	Deploy with SageMaker (What We Actually Do):

# Deploy trained model as SageMaker endpoint
python deployment/model_deployment.py

# Model artifacts automatically retrieved from S3
# SageMaker endpoint created for production inference
# FastAPI server connects via SageMaker runtime API

‚∏ª

7. BitsAndBytes vs Unsloth (For Fine-Tuning)

BitsAndBytes (Colab)
	‚Ä¢	Strengths:
	‚Ä¢	Works with Hugging Face templates out of the box.
	‚Ä¢	Prebuilt Colab notebooks available (many models have ready scripts).
	‚Ä¢	Fast on T4, A100, L4 GPUs.
	‚Ä¢	Streams models from Hugging Face Hub (no manual downloads).
	‚Ä¢	Downsides:
	‚Ä¢	Not optimized for Apple Silicon (for local training).
	‚Ä¢	Session limits (12 hours).

Unsloth (Mac)
	‚Ä¢	Strengths:
	‚Ä¢	Optimized for Apple Silicon (M1/M2/M3) ‚Äî much faster and leaner than BitsAndBytes locally.
	‚Ä¢	Handles 4-bit quantization automatically (no manual tuning).
	‚Ä¢	Useful if you want to continue training locally after Colab.
	‚Ä¢	Downsides:
	‚Ä¢	Less documentation, more manual steps to export.
	‚Ä¢	Slower than an A100 GPU if doing full fine-tunes.

‚∏ª

8. Final Deliverables (What We Actually Have)
	‚Ä¢	After Colab training:

adapter_model.safetensors
adapter_config.json


	‚Ä¢	After merging on Mac:

config.json
tokenizer.json
model.safetensors


	‚Ä¢	After quantization (final):

fine_tuned_model.gguf



This GGUF model is what you‚Äôll run via Ollama or llama.cpp.

‚∏ª

9. Our Implementation: QLoRA with Unsloth

What We Actually Used:
	‚Ä¢	Base Model: Qwen2.5-3B-Instruct (instruction-tuned, 3.09B parameters)
	‚Ä¢	Method: QLoRA (Quantized LoRA) via Unsloth framework
	‚Ä¢	Hardware: Google Colab (T4/A100 GPU) for training
	‚Ä¢	Dataset: 52K instruction samples from Jigsaw (training_dataset_colab.json)
	‚Ä¢	Training Notebook: finetunning/qlora_unsloth.ipynb

Training Process:
	1.	Load Qwen2.5-3B-Instruct in 4-bit quantization (NF4)
	2.	Attach LoRA adapters (r=16, alpha=32, dropout=0.05)
	3.	Train on algospeak classification task
	4.	Export quantized GGUF: unsloth.Q4_K_M.gguf (1.9GB)
	5.	Deploy via SageMaker endpoints for production scaling

Why This Works:
	‚Ä¢	Memory Efficient: 4-bit quantization fits on SageMaker GPU instances
	‚Ä¢	Fast Training: QLoRA optimizations for efficient fine-tuning
	‚Ä¢	Production Ready: SageMaker endpoints with auto-scaling
	‚Ä¢	Cost Effective: Pay-per-use training, fixed-cost inference

Results:
	‚Ä¢	Model: Fine-tuned Qwen2.5-3B with QLoRA adapters
	‚Ä¢	Inference Speed: ~50-100ms on SageMaker ml.g4dn.xlarge
	‚Ä¢	Accuracy: Handles both direct language and algospeak
	‚Ä¢	Integration: REST API via SageMaker runtime

‚∏ª

## üéØ FINE-TUNING SUMMARY

**Why QLoRA + SageMaker is Perfect for Our Production Project:**

‚úÖ **Resource Constraints Solved:**
- Local development + AWS SageMaker ‚Üí Production LLM
- No need for expensive A100 clusters or full fine-tuning
- 4-bit quantization made 3B model fit in SageMaker instances

‚úÖ **Training Success:**
- QLoRA: Only trained adapters (MB), not full model (GB)  
- SageMaker: Automatic HuggingFace model download and training
- 52K samples ‚Üí High-quality algospeak classification

‚úÖ **Production Deployment:**
- SageMaker endpoints with auto-scaling
- Direct integration with FastAPI backend
- Enterprise-grade reliability and monitoring

**Current Implementation:**
- `training/train_script.py` - SageMaker training worker
- `training/model_training.py` - Training orchestrator
- `data/dataset/training_dataset.json` - 52K instruction samples
- `deployment/model_deployment.py` - SageMaker endpoint deployment

**Result:** Production-grade content moderation system with complete MLOps pipeline!

‚∏ª
